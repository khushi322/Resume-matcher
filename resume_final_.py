# -*- coding: utf-8 -*-
"""resume final .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1exI28dgv3HgKNGAr2nCH7TrrAx_7QIS1
"""

!pip install plotly
# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# Check if required libraries are already installed
import importlib.util
import sys
import subprocess

required_libraries = ['numpy', 'PyPDF2', 'textacy', 'transformers', 'keybert', 'requests', 'requests-html', 'lxml_html_clean']
missing_libraries = [lib for lib in required_libraries if importlib.util.find_spec(lib) is None]

if missing_libraries:
    print(f"Installing missing libraries: {missing_libraries}")
    subprocess.run(['pip', 'install', '--upgrade'] + missing_libraries, check=True)

# Install SpaCy model if not already present
try:
    import spacy
    spacy.load('en_core_web_sm')
except:
    subprocess.run(['python', '-m', 'spacy', 'download', 'en_core_web_sm'], check=True)

import PyPDF2
import textacy
import textacy.preprocessing as tprep
from transformers import AutoTokenizer, AutoModel
from keybert import KeyBERT
import requests
from requests_html import HTMLSession
from google.colab import files
import re
import torch
import numpy as np

# Initialize KeyBERT, BERT, and SpaCy models
kw_model = KeyBERT(model='distilbert-base-nli-mean-tokens')
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')
nlp = spacy.load('en_core_web_sm')

# Function to fetch and clean job description from a URL
def fetch_job_description_from_url(url):
    try:
        session = HTMLSession()
        response = session.get(url, timeout=15)
        response.html.render(timeout=20, sleep=3)  # Render JavaScript content
        # Target job description sections
        text_elements = response.html.find('p,div,li,h1,h2,h3,span[class*="description"],span[class*="job"],span[class*="responsibilities"],span[class*="requirements"],span[class*="qualifications"],div[class*="description"],div[class*="job"],div[class*="responsibilities"],div[class*="requirements"],div[class*="qualifications"]')
        if not text_elements:
            text_elements = response.html.find('p,div,li,span')
        text = ' '.join([elem.text.strip() for elem in text_elements if elem.text.strip()])
        # Remove boilerplate content
        irrelevant_patterns = r'\b(cookie|privacy|policy|linkedin|sign|agree|agreement|email|jobid|jobs|apply|footer|copyright|career|posting|terms|conditions|equal opportunity|company|team|about us)\b|http[s]?://\S+|www\.\S+|\d{4,}'
        text = re.sub(irrelevant_patterns, '', text, flags=re.IGNORECASE)
        text = re.sub(r'\s+', ' ', text).strip()
        if len(text.split()) < 50:
            return "", f"Warning: URL content too short ({len(text.split())} words) or not a job description."
        return text, ""
    except Exception as e:
        return "", f"Error fetching URL {url}: {e}"
    finally:
        session.close()

# Function to extract text from PDF
def extract_text_from_pdf(file_path):
    try:
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page in reader.pages:
                page_text = page.extract_text() or ''
                text += page_text
            return text
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""

# Function to preprocess text
def preprocess_text(text):
    if not text or not isinstance(text, str):
        return ""
    # Normalize text using textacy
    text = tprep.normalize.unicode(text)
    text = tprep.normalize.whitespace(text)
    text = tprep.remove.punctuation(text)
    # Remove URLs, numbers, and irrelevant terms
    text = re.sub(r'http[s]?://\S+|www\.\S+|\d{4,}|\b(cookie|privacy|linkedin|sign|agree|agreement|email|jobid|jobs|apply|footer|copyright|career|posting|company|team)\b', '', text, flags=re.IGNORECASE)
    # Tokenize and remove stop words
    doc = nlp(text)
    tokens = [token.text.lower() for token in doc if not token.is_stop and len(token.text) > 3]
    return ' '.join(tokens) if tokens else ""

# Function to split text into sentences
def split_sentences(text):
    if not text or not isinstance(text, str):
        return []
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents if len(sent.text.strip()) > 5]

# Function to extract skills dynamically
def extract_skills(resume_text, job_text):
    if not resume_text or not job_text:
        return []
    # Extract key terms from job description using textacy
    job_doc = textacy.make_spacy_doc(job_text, lang='en_core_web_sm')
    job_terms = textacy.extract.keyterms.textrank(job_doc, normalize='lower', topn=30)
    skills_list = [term for term, _ in job_terms if len(term) > 3 and not re.match(r'^\d+$', term) and not re.search(r'job|apply|career|posting|company|team', term, re.IGNORECASE)]
    # Check for skills in resume
    resume_doc = textacy.make_spacy_doc(resume_text, lang='en_core_web_sm')
    extracted_skills = [token.text.lower() for token in resume_doc if token.text.lower() in skills_list]
    return list(set(extracted_skills))

# Function to extract keywords using KeyBERT
def extract_keywords(resume_text, job_text):
    if not resume_text or not job_text:
        return [], [], []

    # Extract keywords from job description
    job_keywords = kw_model.extract_keywords(job_text, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=30)
    job_keywords = [(kw, score) for kw, score in job_keywords if len(kw) > 3 and not re.match(r'^\d+$', kw) and not re.search(r'job|apply|career|posting|company|team', kw, re.IGNORECASE)]

    # Extract keywords from resume
    resume_keywords = kw_model.extract_keywords(resume_text, keyphrase_ngram_range=(1, 3), stop_words='english', top_n=30)
    resume_keywords = [kw for kw, score in resume_keywords if len(kw) > 3 and not re.match(r'^\d+$', kw) and not re.search(r'job|apply|career|posting|company|team', kw, re.IGNORECASE)]

    matching_keywords = [(kw, score) for kw, score in job_keywords if kw in resume_keywords]
    missing_keywords = [(kw, score) for kw, score in job_keywords if kw not in resume_keywords]
    overused_keywords = [kw for kw in resume_keywords if kw not in [k[0] for k in job_keywords]]

    return matching_keywords, missing_keywords, overused_keywords

# Function to compute BERT-based semantic similarity
def compute_semantic_similarity(resume_text, job_text):
    if not resume_text or not job_text:
        return 0.0
    # Split into sentences using spacy
    resume_sentences = split_sentences(resume_text)
    job_sentences = split_sentences(job_text)
    if not resume_sentences or not job_sentences:
        print("Warning: No sentences detected in resume or job description.")
        return 0.0

    # Encode sentences
    resume_inputs = tokenizer(resume_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)
    job_inputs = tokenizer(job_sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)

    with torch.no_grad():
        resume_outputs = model(**resume_inputs)
        job_outputs = model(**job_inputs)

    # Mean pool embeddings
    resume_embeddings = resume_outputs.last_hidden_state.mean(dim=1)
    job_embeddings = job_outputs.last_hidden_state.mean(dim=1)

    # Compute cosine similarity
    similarity = torch.nn.functional.cosine_similarity(resume_embeddings.mean(dim=0), job_embeddings.mean(dim=0), dim=0).item()
    return max(similarity, 0.0)

# Function to check ATS formatting issues
def check_formatting_issues(resume_text):
    issues = []
    standard_headings = ['education', 'experience', 'skills', 'projects', 'certifications', 'work experience']
    headings_found = [h.lower() for h in re.findall(r'^\s*(\w+\s*\w*)\s*:', resume_text, re.MULTILINE)]
    non_standard_headings = [h for h in headings_found if h not in standard_headings]
    if non_standard_headings:
        issues.append(f"Non-standard headings detected: {', '.join(non_standard_headings)}.")

    word_count = len(resume_text.split())
    if word_count > 600:
        issues.append(f"Resume exceeds optimal length ({word_count} words).")
    elif word_count < 300:
        issues.append(f"Resume is too short ({word_count} words).")

    if '|' in resume_text or '---' in resume_text or '\t' in resume_text:
        issues.append("Complex formatting detected.")

    font_indicators = re.findall(r'font-family:.*?(Arial|Times New Roman|Calibri|Garamond|Helvetica)', resume_text, re.IGNORECASE)
    if not font_indicators:
        issues.append("Non-standard or unspecified fonts detected.")

    return issues

# Function to calculate ATS score
def calculate_ats_score(matching_keywords, jd_keywords, formatting_issues, resume_text, semantic_score):
    keyword_score = min(len(matching_keywords) / max(len(jd_keywords), 1) * 70, 70)  # 70% weight
    formatting_score = (1 - len(formatting_issues) / 5) * 5 if formatting_issues else 5  # 5% weight
    content_score = min(len(resume_text.split()) / 400 * 5, 5) if len(resume_text.split()) <= 600 else 2.5  # 5% weight
    semantic_score = semantic_score * 20  # 20% weight
    total_score = int(keyword_score + formatting_score + content_score + semantic_score)
    print(f"Debug: Keyword Score: {keyword_score:.2f}/70, Formatting Score: {formatting_score:.2f}/5, Content Score: {content_score:.2f}/5, Semantic Score: {semantic_score:.2f}/20")
    return max(min(total_score, 100), 0)

# Main execution
def main():
    try:
        print("Enter a job description (text, e.g., 'Data Scientist: Proficient in Python...') or a URL:")
        job_input = input().strip()
        if not job_input or len(job_input.split()) < 10:
            print("Error: Job description too short or empty. Using default.")
            job_description = "Data Scientist: Proficient in Python, SQL, machine learning, Tableau, and data analysis. Responsibilities include building predictive models, analyzing large datasets, and creating data visualizations. Experience with AWS and statistical modeling preferred."
        else:
            if job_input.startswith('http'):
                print("Fetching job description from URL...")
                job_description, warning = fetch_job_description_from_url(job_input)
                if warning:
                    print(warning)
                if not job_description:
                    print("Error: No valid job description from URL. Using default.")
                    job_description = "Data Scientist: Proficient in Python, SQL, machine learning, Tableau, and data analysis."
            else:
                job_description = job_input

        print("Please upload a single PDF resume file:")
        uploaded = files.upload()
        if len(uploaded) != 1 or not list(uploaded.keys())[0].endswith('.pdf'):
            raise ValueError("Please upload exactly one PDF file.")

        resume_file = list(uploaded.keys())[0]
        resume_text = extract_text_from_pdf(resume_file)
        if not resume_text or len(resume_text.split()) < 50:
            raise ValueError("Could not extract text from the resume or resume is too short. Ensure it's a text-based PDF.")

        resume_processed = preprocess_text(resume_text)
        job_processed = preprocess_text(job_description)
        if not resume_processed or not job_processed:
            raise ValueError("Empty resume or job description after preprocessing.")

        # Extract skills and keywords
        skills = extract_skills(resume_text, job_description)
        matching_keywords, missing_keywords, overused_keywords = extract_keywords(resume_text, job_description)

        # Compute semantic similarity
        semantic_similarity = compute_semantic_similarity(resume_text, job_description)

        # Check formatting issues
        formatting_issues = check_formatting_issues(resume_text)

        # Calculate ATS score
        ats_score = calculate_ats_score(matching_keywords, matching_keywords + missing_keywords, formatting_issues, resume_text, semantic_similarity)

        # Display ATS score, skills, and keywords
        print(f"\nATS Match Score: {ats_score}%")
        print(f"Extracted Skills: {', '.join(skills) if skills else 'None'}")
        print(f"Matching Keywords: {', '.join([kw[0] for kw in matching_keywords]) if matching_keywords else 'None'}")
        print(f"Missing Keywords: {', '.join([kw[0] for kw in missing_keywords]) if missing_keywords else 'None'}")

        return ats_score, skills, matching_keywords, missing_keywords, overused_keywords, formatting_issues


    except Exception as e:
        print(f"An error occurred: {e}")
        return None, None, None, None, None, None


if __name__ == "__main__":
    ats_score, skills, matching_keywords, missing_keywords, overused_keywords, formatting_issues = main()

import pandas as pd
import plotly.graph_objects as go
from IPython.display import display, Markdown, HTML
import random

# ==== ATS Score Gauge ====
fig = go.Figure(go.Indicator(
    mode="gauge+number",
    value=ats_score,
    domain={'x': [0, 1], 'y': [0, 1]},
    title={'text': "ATS Match Score (%)", 'font': {'size': 24}},
    gauge={
        'axis': {'range': [0, 100]},
        'bar': {'color': "darkblue"},
        'steps': [
            {'range': [0, 50], 'color': "red"},
            {'range': [50, 75], 'color': "yellow"},
            {'range': [75, 100], 'color': "green"},
        ],
    }
))
fig.update_layout(height=300)
fig.show()

# ==== Skills as Color Tags ====
display(Markdown("## ‚úÖ Extracted Skills"))
if skills:
    tag_html = '<div style="display:flex; flex-wrap:wrap; gap:10px;">'
    for skill in skills:
        color = f"hsl({random.randint(0, 360)}, 70%, 70%)"
        tag_html += f'<span style="padding:5px 10px; background-color:{color}; border-radius:15px;">{skill}</span>'
    tag_html += '</div>'
    display(HTML(tag_html))
else:
    display(Markdown("`No skills extracted.`"))

# ==== Matching & Missing Keywords Summary (Bar Chart) ====
matching_count = len(matching_keywords)
missing_count = len(missing_keywords)
overused_count = len(overused_keywords)

fig2 = go.Figure(data=[
    go.Bar(name='Matching Keywords', x=["Keywords"], y=[matching_count], marker_color='green'),
    go.Bar(name='Missing Keywords', x=["Keywords"], y=[missing_count], marker_color='orange'),
    go.Bar(name='Overused Keywords', x=["Keywords"], y=[overused_count], marker_color='red')
])
fig2.update_layout(title="üîç Keyword Match Summary", barmode='group', height=400)
fig2.show()

# ==== Matching Keywords Table ====
if matching_keywords:
    display(Markdown("## ‚úÖ Matching Keywords"))
    match_df = pd.DataFrame(matching_keywords, columns=["Keyword", "Relevance Score"])
    match_df["Relevance Score (%)"] = (match_df["Relevance Score"] * 100).round(1)
    display(match_df.drop(columns="Relevance Score"))
else:
    display(Markdown("`No matching keywords found.`"))

# ==== Missing Keywords Table ====
if missing_keywords:
    display(Markdown("## ‚ùå Missing Keywords"))
    missing_df = pd.DataFrame(missing_keywords, columns=["Keyword", "Relevance Score"])
    missing_df["Relevance Score (%)"] = (missing_df["Relevance Score"] * 100).round(1)
    display(missing_df.drop(columns="Relevance Score"))

# ==== Overused Keywords ====
if overused_keywords:
    display(Markdown("## ‚ö†Ô∏è Overused Keywords"))
    overused_df = pd.DataFrame(overused_keywords, columns=["Overused Keyword"])
    display(overused_df)

# ==== Formatting Issues ====
display(Markdown("## üìù Resume Formatting Issues"))
if formatting_issues:
    for issue in formatting_issues:
        display(HTML(f'<div style="background-color:#ffe6e6; padding:10px; margin:5px 0; border-left:5px solid red;"><strong>‚ö†Ô∏è {issue}</strong></div>'))
else:
    display(HTML('<div style="background-color:#e6ffe6; padding:10px; border-left:5px solid green;"><strong>‚úÖ No formatting issues detected.</strong></div>'))